// Author: AI Assistant / Claude
// Generated by Claude for GPU-MPC optimizations
// Copyright:
// 
// Copyright (c) 2024 Microsoft Research
// 
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.

#include "mpc_benchmark.h"
#include "fss/optimizations/fss_memory_patterns.cuh"
#include "fss/optimizations/dcf_kernel_tuning.cuh"
#include "fss/gpu_aes_shm.cuh"

#include <cuda_runtime.h>
#include <iostream>
#include <vector>

using namespace fss::optimizations;
using namespace dcf::optimizations;

constexpr int TEST_SIZE = 4096;
constexpr int FSS_TABLE_SIZE = 256;
constexpr int DCF_DEPTH = 16;

// Baseline FSS table lookup
template<typename T>
__global__ void baselineFSS(const T* table, const u32* indices, T* results, int n) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < n) {
        results[tid] = table[indices[tid] % FSS_TABLE_SIZE];
    }
}

// Test coalesced FSS lookup
template<typename T>
__global__ void testCoalescedFSS(const T* table, const u32* indices, T* results, int n) {
    coalescedFSSLookup<T, FSS_TABLE_SIZE>(table, indices, results, n);
}

// Test shared memory FSS lookup  
template<typename T>
__global__ void testSharedFSS(const T* table, const u32* indices, T* results, int n) {
    sharedFSSLookup<T, FSS_TABLE_SIZE>(table, indices, results, n);
}

// Test vectorized FSS lookup
template<typename T>
__global__ void testVectorizedFSS(const T* table, const u32* indices, T* results, int n) {
    vectorizedFSSLookup<T>(table, indices, results, n);
}

// Baseline DCF evaluation
template<typename T>
__global__ void baselineDCF(const T* input, const AESBlock* keys, u32* output, 
                           int n, int depth, int party, AESGlobalContext aes_global) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < n) {
        // Simple placeholder DCF computation
        T val = input[tid];
        AESBlock state = keys[tid];
        for (int i = 0; i < depth; i++) {
            state = state ^ (val << i);
        }
        output[tid] = state & 1;
    }
}

// Test optimized DCF kernel using the new implementation
template<typename T>
void launchOptimizedDCF(const T* input, const AESBlock* keys, u32* output,
                       int n, int depth, int party, AESGlobalContext aes_global) {
    // Use fixed 256 thread blocks for cryptographic security
    dim3 grid((n + DCF_BLOCK_SIZE - 1) / DCF_BLOCK_SIZE);
    dim3 block(DCF_BLOCK_SIZE);
    
    optimizedDCFKernel<T><<<grid, block>>>(input, keys, output, n, depth, party, aes_global);
}

// Test memory-optimized DCF kernel
template<typename T>
void launchMemoryOptimizedDCF(const T* input, const AESBlock* keys, u32* output,
                             int n, int depth, int party, AESGlobalContext aes_global) {
    dim3 grid((n + DCF_BLOCK_SIZE - 1) / DCF_BLOCK_SIZE);
    dim3 block(DCF_BLOCK_SIZE);
    
    memoryOptimizedDCFKernel<T><<<grid, block>>>(input, keys, output, n, depth, party, aes_global);
}

// Simple timer
class Timer {
    cudaEvent_t start, stop;
public:
    Timer() { cudaEventCreate(&start); cudaEventCreate(&stop); }
    ~Timer() { cudaEventDestroy(start); cudaEventDestroy(stop); }
    void begin() { cudaEventRecord(start); }
    float end() { 
        cudaEventRecord(stop); 
        cudaEventSynchronize(stop);
        float ms;
        cudaEventElapsedTime(&ms, start, stop);
        return ms;
    }
};

// Run FSS benchmarks
void benchmarkFSS() {
    std::cout << "FSS Memory Pattern Optimization Tests:" << std::endl;
    
    // Allocate memory with power-of-2 table size for MPC security
    static_assert((FSS_TABLE_SIZE & (FSS_TABLE_SIZE - 1)) == 0, "Table size must be power of 2");
    
    u64 *d_table, *d_results_baseline, *d_results_opt;
    u32 *d_indices;
    
    cudaMalloc(&d_table, FSS_TABLE_SIZE * sizeof(u64));
    cudaMalloc(&d_indices, TEST_SIZE * sizeof(u32));
    cudaMalloc(&d_results_baseline, TEST_SIZE * sizeof(u64));
    cudaMalloc(&d_results_opt, TEST_SIZE * sizeof(u64));
    
    // Initialize data with cryptographic pattern
    std::vector<u64> host_table(FSS_TABLE_SIZE);
    std::vector<u32> host_indices(TEST_SIZE);
    
    for (int i = 0; i < FSS_TABLE_SIZE; i++) {
        host_table[i] = 0x4142434445464748ULL + i; // Pattern for testing
    }
    for (int i = 0; i < TEST_SIZE; i++) {
        host_indices[i] = i % FSS_TABLE_SIZE; // Controlled access pattern
    }
    
    cudaMemcpy(d_table, host_table.data(), FSS_TABLE_SIZE * sizeof(u64), cudaMemcpyHostToDevice);
    cudaMemcpy(d_indices, host_indices.data(), TEST_SIZE * sizeof(u32), cudaMemcpyHostToDevice);
    
    Timer timer;
    const int RUNS = 10;
    
    // Test coalesced lookup with cryptographic constraints
    float baseline_time = 0, opt_time = 0;
    
    for (int i = 0; i < RUNS; i++) {
        timer.begin();
        baselineFSS<<<(TEST_SIZE+255)/256, 256>>>(d_table, d_indices, d_results_baseline, TEST_SIZE);
        baseline_time += timer.end();
        
        timer.begin();
        testCoalescedFSS<<<(TEST_SIZE+255)/256, 256>>>(d_table, d_indices, d_results_opt, TEST_SIZE);
        opt_time += timer.end();
    }
    
    float speedup = baseline_time / opt_time;
    std::cout << "  Coalesced FSS: " << baseline_time/RUNS << "ms -> " << opt_time/RUNS 
              << "ms (speedup: " << speedup << "x)" << std::endl;
    
    // Test shared memory lookup with bank conflict avoidance
    opt_time = 0;
    for (int i = 0; i < RUNS; i++) {
        timer.begin();
        testSharedFSS<<<(TEST_SIZE+255)/256, 256>>>(d_table, d_indices, d_results_opt, TEST_SIZE);
        opt_time += timer.end();
    }
    speedup = baseline_time / opt_time;
    std::cout << "  Shared Memory FSS: " << opt_time/RUNS << "ms (speedup: " << speedup << "x)" << std::endl;
    
    // Test vectorized lookup with alignment considerations
    opt_time = 0;
    for (int i = 0; i < RUNS; i++) {
        timer.begin();
        testVectorizedFSS<<<(TEST_SIZE/4+255)/256, 256>>>(d_table, d_indices, d_results_opt, TEST_SIZE);
        opt_time += timer.end();
    }
    speedup = baseline_time / opt_time;
    std::cout << "  Vectorized FSS: " << opt_time/RUNS << "ms (speedup: " << speedup << "x)" << std::endl;
    
    // Verify correctness of optimizations
    std::vector<u64> results_baseline(TEST_SIZE), results_opt(TEST_SIZE);
    cudaMemcpy(results_baseline.data(), d_results_baseline, TEST_SIZE * sizeof(u64), cudaMemcpyDeviceToHost);
    cudaMemcpy(results_opt.data(), d_results_opt, TEST_SIZE * sizeof(u64), cudaMemcpyDeviceToHost);
    
    bool correct = true;
    for (int i = 0; i < TEST_SIZE && correct; i++) {
        if (results_baseline[i] != results_opt[i]) {
            correct = false;
        }
    }
    std::cout << "  Correctness check: " << (correct ? "PASSED" : "FAILED") << std::endl;
    
    // Cleanup
    cudaFree(d_table);
    cudaFree(d_indices);
    cudaFree(d_results_baseline);
    cudaFree(d_results_opt);
}

// Run DCF benchmarks
void benchmarkDCF() {
    std::cout << "DCF Kernel Tuning Optimization Tests:" << std::endl;
    
    // Allocate memory with cryptographically aligned sizes
    u64 *d_input;
    AESBlock *d_keys;
    u32 *d_output_baseline, *d_output_opt, *d_output_mem_opt;
    
    cudaMalloc(&d_input, TEST_SIZE * sizeof(u64));
    cudaMalloc(&d_keys, TEST_SIZE * (DCF_DEPTH + 1) * sizeof(AESBlock));
    cudaMalloc(&d_output_baseline, TEST_SIZE * sizeof(u32));
    cudaMalloc(&d_output_opt, TEST_SIZE * sizeof(u32));
    cudaMalloc(&d_output_mem_opt, TEST_SIZE * sizeof(u32));
    
    // Initialize data with realistic cryptographic patterns
    std::vector<u64> host_input(TEST_SIZE);
    std::vector<AESBlock> host_keys(TEST_SIZE * (DCF_DEPTH + 1));
    
    for (int i = 0; i < TEST_SIZE; i++) {
        host_input[i] = (static_cast<u64>(i) << 32) | (i ^ 0x12345678); // Realistic input pattern
    }
    for (int i = 0; i < TEST_SIZE * (DCF_DEPTH + 1); i++) {
        host_keys[i] = static_cast<AESBlock>(0x0123456789ABCDEFULL) + i; // Key pattern
    }
    
    cudaMemcpy(d_input, host_input.data(), TEST_SIZE * sizeof(u64), cudaMemcpyHostToDevice);
    cudaMemcpy(d_keys, host_keys.data(), TEST_SIZE * (DCF_DEPTH + 1) * sizeof(AESBlock), cudaMemcpyHostToDevice);
    
    // Initialize AES context
    AESGlobalContext aes_ctx;
    initAESContext(&aes_ctx);
    
    Timer timer;
    const int RUNS = 5;
    
    float baseline_time = 0, opt_time = 0, mem_opt_time = 0;
    
    // Benchmark baseline DCF
    for (int i = 0; i < RUNS; i++) {
        timer.begin();
        baselineDCF<<<(TEST_SIZE+255)/256, 256>>>(d_input, d_keys, d_output_baseline, 
                                                 TEST_SIZE, DCF_DEPTH, 0, aes_ctx);
        baseline_time += timer.end();
    }
    
    // Benchmark optimized DCF with warp primitives
    for (int i = 0; i < RUNS; i++) {
        timer.begin();
        launchOptimizedDCF<u64>(d_input, d_keys, d_output_opt, TEST_SIZE, DCF_DEPTH, 0, aes_ctx);
        opt_time += timer.end();
    }
    
    // Benchmark memory-optimized DCF for large trees
    for (int i = 0; i < RUNS; i++) {
        timer.begin();
        launchMemoryOptimizedDCF<u64>(d_input, d_keys, d_output_mem_opt, TEST_SIZE, DCF_DEPTH, 0, aes_ctx);
        mem_opt_time += timer.end();
    }
    
    std::cout << "  Baseline DCF: " << baseline_time/RUNS << "ms" << std::endl;
    std::cout << "  Warp-Optimized DCF: " << opt_time/RUNS << "ms" << std::endl;
    std::cout << "  Memory-Optimized DCF: " << mem_opt_time/RUNS << "ms" << std::endl;
    
    float warp_speedup = baseline_time / opt_time;
    float mem_speedup = baseline_time / mem_opt_time;
    std::cout << "  Warp Optimization Speedup: " << warp_speedup << "x" << std::endl;
    std::cout << "  Memory Optimization Speedup: " << mem_speedup << "x" << std::endl;
    
    // Verify correctness of optimizations
    std::vector<u32> results_baseline(TEST_SIZE), results_opt(TEST_SIZE), results_mem_opt(TEST_SIZE);
    cudaMemcpy(results_baseline.data(), d_output_baseline, TEST_SIZE * sizeof(u32), cudaMemcpyDeviceToHost);
    cudaMemcpy(results_opt.data(), d_output_opt, TEST_SIZE * sizeof(u32), cudaMemcpyDeviceToHost);
    cudaMemcpy(results_mem_opt.data(), d_output_mem_opt, TEST_SIZE * sizeof(u32), cudaMemcpyDeviceToHost);
    
    bool warp_correct = true, mem_correct = true;
    for (int i = 0; i < TEST_SIZE; i++) {
        if (results_baseline[i] != results_opt[i]) warp_correct = false;
        if (results_baseline[i] != results_mem_opt[i]) mem_correct = false;
    }
    
    std::cout << "  Warp Optimization Correctness: " << (warp_correct ? "PASSED" : "FAILED") << std::endl;
    std::cout << "  Memory Optimization Correctness: " << (mem_correct ? "PASSED" : "FAILED") << std::endl;
    
    // Report memory usage efficiency
    size_t key_memory = TEST_SIZE * (DCF_DEPTH + 1) * sizeof(AESBlock);
    double gb_accessed = (key_memory / (1024.0 * 1024.0 * 1024.0)) * RUNS;
    double bandwidth_baseline = gb_accessed / (baseline_time / 1000.0);
    double bandwidth_opt = gb_accessed / (opt_time / 1000.0);
    
    std::cout << "  Memory Bandwidth (baseline): " << bandwidth_baseline << " GB/s" << std::endl;
    std::cout << "  Memory Bandwidth (optimized): " << bandwidth_opt << " GB/s" << std::endl;
    
    // Cleanup
    cudaFree(d_input);
    cudaFree(d_keys);
    cudaFree(d_output_baseline);
    cudaFree(d_output_opt);
    cudaFree(d_output_mem_opt);
    freeAESGlobalContext(&aes_ctx);
}

// Main benchmark function
void runFSSOptimizationBenchmarks() {
    std::cout << "===== FSS Optimization Benchmark =====" << std::endl;
    std::cout << "Problem Size: " << TEST_SIZE << std::endl;
    std::cout << "FSS Table Size: " << FSS_TABLE_SIZE << std::endl;
    std::cout << "DCF Tree Depth: " << DCF_DEPTH << std::endl;
    std::cout << std::endl;
    
    try {
        benchmarkFSS();
        std::cout << std::endl;
        benchmarkDCF();
    } catch (...) {
        std::cout << "Note: GPU kernels require CUDA hardware to run" << std::endl;
    }
    
    std::cout << "=======================================" << std::endl;
}

// Task registration
void fssOptimizationBenchmarkTask(int party, const std::string& peer_ip, int threads) {
    (void)party; (void)peer_ip; (void)threads;
    runFSSOptimizationBenchmarks();
}

static bool registered = registerTask("fss_opt", fssOptimizationBenchmarkTask);

// Note: main() is provided by mpc_benchmark.cu