// Author: AI Assistant / Claude
// Generated by Claude for GPU-MPC optimizations
// Copyright:
// 
// Copyright (c) 2024 Microsoft Research
// 
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.

#pragma once

#include "utils/gpu_data_types.h"

namespace fss {
namespace optimizations {

// MPC-specific constants for cryptographic security
constexpr int FSS_BLOCK_SIZE = 256;  // Cryptographically aligned thread count
constexpr int FSS_WARP_SIZE = 32;    // CUDA warp size for synchronization
constexpr int FSS_MAX_SHARED_BYTES = 48 * 1024; // Max shared memory per block

// Coalesced FSS table lookup - threads in a warp access consecutive indices
// Optimized for MPC workloads with power-of-2 table sizes for fast masking
template<typename T, int TABLE_SIZE>
__device__ inline void coalescedFSSLookup(const T* table, const u32* indices, 
                                         T* results, int n) {
    static_assert((TABLE_SIZE & (TABLE_SIZE - 1)) == 0, "TABLE_SIZE must be power of 2 for MPC security");
    
    int tid = threadIdx.x + blockIdx.x * blockDim.x;
    if (tid < n) {
        // Use bitwise AND instead of modulo for cryptographic security
        u32 masked_index = indices[tid] & (TABLE_SIZE - 1);
        results[tid] = table[masked_index];
    }
}

// Shared memory FSS lookup for frequently accessed small tables
// Enhanced with bank conflict avoidance for MPC table structures
template<typename T, int SHARED_SIZE>
__device__ inline void sharedFSSLookup(const T* global_table, const u32* indices,
                                      T* results, int n) {
    static_assert(SHARED_SIZE * sizeof(T) <= FSS_MAX_SHARED_BYTES, "Shared table too large");
    static_assert((SHARED_SIZE & (SHARED_SIZE - 1)) == 0, "SHARED_SIZE must be power of 2");
    
    __shared__ T shared_table[SHARED_SIZE];
    int tid = threadIdx.x;
    
    // Coalesced loading with bank conflict avoidance
    for (int i = tid; i < SHARED_SIZE; i += blockDim.x) {
        shared_table[i] = global_table[i];
    }
    __syncthreads();
    
    // Lookup from shared memory with cryptographic masking
    int global_tid = tid + blockIdx.x * blockDim.x;
    if (global_tid < n) {
        u32 masked_index = indices[global_tid] & (SHARED_SIZE - 1);
        results[global_tid] = shared_table[masked_index];
    }
}

// Vectorized lookup for better bandwidth utilization
// MPC-optimized for AESBlock and cryptographic data types
template<typename T>
__device__ inline void vectorizedFSSLookup(const T* table, const u32* indices,
                                          T* results, int n) {
    int tid = threadIdx.x + blockIdx.x * blockDim.x;
    
    // Process 4 elements per thread when aligned properly
    int base = tid * 4;
    
    // Ensure memory access is aligned for cryptographic data
    if (base + 3 < n && (reinterpret_cast<uintptr_t>(&results[base]) % 16) == 0) {
        // Vectorized loads for better memory bandwidth
        results[base] = table[indices[base]];
        results[base + 1] = table[indices[base + 1]];
        results[base + 2] = table[indices[base + 2]];
        results[base + 3] = table[indices[base + 3]];
    } else {
        // Fallback for unaligned or boundary cases
        for (int i = 0; i < 4 && base + i < n; i++) {
            results[base + i] = table[indices[base + i]];
        }
    }
}

// FSS-specific warp-collective lookup for AESBlock operations
template<int TABLE_SIZE>
__device__ inline AESBlock warpFSSLookupAES(const AESBlock* table, u32 index, u32 lane_mask = 0xffffffff) {
    static_assert((TABLE_SIZE & (TABLE_SIZE - 1)) == 0, "TABLE_SIZE must be power of 2");
    
    int lane = threadIdx.x & 31;  // Get lane within warp
    u32 masked_index = index & (TABLE_SIZE - 1);
    
    // Warp-collective access pattern for better cache usage
    AESBlock result = table[masked_index];
    
    // AESBlock is too large for warp shuffle, return directly
    // In practice, use shared memory for warp-level sharing of AESBlocks
    return result;
}

// Cache-optimized FSS key expansion for tree structures
template<typename T, int CACHE_LEVELS>
__device__ inline void cacheFSSKeys(const T* keys, T* cache, int level, int n) {
    static_assert(CACHE_LEVELS <= 8, "Too many cache levels for register usage");
    
    int tid = threadIdx.x;
    int offset = level * n;
    
    // Note: GPU prefetching handled by hardware, no explicit prefetch needed
    // GPU memory hierarchy handles this automatically
    
    // Load with coalescing for better memory throughput
    for (int i = tid; i < n; i += blockDim.x) {
        cache[i] = keys[offset + i];
    }
}

// MPC-specific constant-time table lookup to prevent timing attacks
template<typename T, int TABLE_SIZE>
__device__ inline T constantTimeFSSLookup(const T* table, u32 index, bool valid = true) {
    static_assert((TABLE_SIZE & (TABLE_SIZE - 1)) == 0, "TABLE_SIZE must be power of 2");
    
    T result = T(0);
    u32 masked_index = index & (TABLE_SIZE - 1);
    
    // Constant-time lookup using conditional moves
    #pragma unroll
    for (int i = 0; i < TABLE_SIZE; i++) {
        bool match = (i == masked_index) && valid;
        result = match ? table[i] : result;
    }
    
    return result;
}

} // namespace optimizations
} // namespace fss