// Author: AI Assistant / Claude
// Generated by Claude for GPU-MPC optimizations
// Copyright:
// 
// Copyright (c) 2024 Microsoft Research
// 
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.

#pragma once

#include "utils/gpu_data_types.h"
#include "fss/gpu_aes_shm.cuh"

namespace dcf {
namespace optimizations {

// DCF-specific constants - 256 threads for cryptographic security
constexpr int DCF_BLOCK_SIZE = 256;
constexpr int DCF_MAX_DEPTH = 64;
constexpr int DCF_WARP_SIZE = 32;
constexpr int DCF_MAX_SHARED_KEYS = 256; // Reduced to fit in shared memory (48KB limit)

// Cryptographically-secure bit extraction for DCF navigation
__device__ inline u8 secureBitExtract(u64 value, int position) {
    // Constant-time bit extraction to prevent timing attacks
    return static_cast<u8>((value >> position) & 1);
}

// Optimized DCF tree traversal using warp primitives
__device__ inline AESBlock warpDCFTraversal(AESBlock state, u8 bit, AESBlock scw, 
                                           int party, AESSharedContext* aes_ctx) {
    const AESBlock notThreeBlock = ~3;
    
    u8 control_bit = state & 1;
    AESBlock clean_state = state & notThreeBlock;
    
    // Use warp voting for branch prediction optimization
    unsigned int branch_votes = __ballot_sync(__activemask(), bit);
    bool branch_divergent = (__popc(branch_votes) > 0) && (__popc(branch_votes) < DCF_WARP_SIZE);
    
    AESBlock left_child, right_child;
    applyAESPRGTwoTimes(aes_ctx, (u32*)&clean_state, bit, 
                       (u32*)&left_child, (u32*)&right_child);
    
    // Extract control bits from SCW for constant-time operation
    AESBlock scw_clean = scw & notThreeBlock;
    u8 left_control = (scw >> 1) & 1; 
    u8 right_control = scw & 1;
    
    // Select child and apply correction in constant time
    AESBlock child = bit ? right_child : left_child;
    u8 control_correction = bit ? right_control : left_control;
    
    // Apply correction based on control bit
    const AESBlock zeroAndAllOne[2] = {0, static_cast<AESBlock>(~0)};
    AESBlock correction_mask = zeroAndAllOne[control_bit];
    AESBlock corrected_scw = scw_clean ^ control_correction;
    
    return child ^ (correction_mask & corrected_scw);
}

// Warp-collaborative DCF key loading for better memory efficiency
__device__ inline void warpLoadDCFKeys(const AESBlock* global_keys, AESBlock* local_cache,
                                      int level, int n, int warp_id) {
    int lane = threadIdx.x & 31;
    int offset = level * n;
    
    // Each warp loads a different segment
    int warp_start = warp_id * DCF_WARP_SIZE;
    int key_idx = warp_start + lane;
    
    if (key_idx < n) {
        local_cache[lane] = global_keys[offset + key_idx];
    }
    
    // Warp-level synchronization
    __syncwarp(__activemask());
}

// Cache-optimized key loading for DCF levels
template<typename T>
__device__ inline void loadDCFKeys(const AESBlock* keys, T* indices, 
                                  AESBlock* cache, int level, int n) {
    int tid = threadIdx.x;
    int offset = level * n;
    
    // Coalesced loading of keys
    for (int i = tid; i < n; i += blockDim.x) {
        cache[i] = keys[offset + i];
    }
}

// Optimized DCF evaluation kernel with fixed 256 threads
// Enhanced with MPC-specific security considerations
template<typename T>
__global__ void optimizedDCFKernel(const T* input, const AESBlock* keys,
                                  u32* output, int n, int depth, int party,
                                  AESGlobalContext aes_global) {
    // blockDim.x is checked at launch time to be DCF_BLOCK_SIZE
    
    AESSharedContext aes_shared;
    loadSbox(&aes_global, &aes_shared);
    
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int warp_id = threadIdx.x / DCF_WARP_SIZE;
    
    // Shared memory for key caching per block
    __shared__ AESBlock key_cache[DCF_MAX_SHARED_KEYS];
    
    if (tid < n) {
        T value = input[tid];
        AESBlock state = keys[tid];
        
        // Tree traversal with optimized navigation and key caching
        for (int level = 0; level < depth; level++) {
            // Load keys collaboratively into shared memory
            if (level * n < DCF_MAX_SHARED_KEYS) {
                warpLoadDCFKeys(keys, &key_cache[level * min(n, DCF_WARP_SIZE)], 
                               level, n, warp_id);
            }
            
            // Extract bit using secure method
            u8 bit = secureBitExtract(value, depth - 1 - level);
            AESBlock scw = (level * n < DCF_MAX_SHARED_KEYS) ? 
                          key_cache[level * min(n, DCF_WARP_SIZE) + (tid % DCF_WARP_SIZE)] :
                          keys[(level + 1) * n + tid];
            
            state = warpDCFTraversal(state, bit, scw, party, &aes_shared);
        }
        
        // Extract final result bit
        output[tid] = state & 1;
    }
}

// Memory-optimized DCF kernel for large trees that don't fit in shared memory
template<typename T>
__global__ void memoryOptimizedDCFKernel(const T* input, const AESBlock* keys,
                                        u32* output, int n, int depth, int party,
                                        AESGlobalContext aes_global) {
    AESSharedContext aes_shared;
    loadSbox(&aes_global, &aes_shared);
    
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (tid < n) {
        T value = input[tid];
        AESBlock state = keys[tid];
        
        // Process tree in chunks to optimize memory access
        constexpr int CHUNK_SIZE = 8;
        
        for (int chunk = 0; chunk < (depth + CHUNK_SIZE - 1) / CHUNK_SIZE; chunk++) {
            int chunk_start = chunk * CHUNK_SIZE;
            int chunk_end = min(chunk_start + CHUNK_SIZE, depth);
            
            // Note: GPU doesn't support __builtin_prefetch, relying on hardware prefetcher
            // Manual prefetching would require separate loads here if needed
            
            for (int level = chunk_start; level < chunk_end; level++) {
                u8 bit = secureBitExtract(value, depth - 1 - level);
                AESBlock scw = keys[(level + 1) * n + tid];
                state = warpDCFTraversal(state, bit, scw, party, &aes_shared);
            }
        }
        
        output[tid] = state & 1;
    }
}

// Register-optimized DCF computation for small depths
__device__ inline u32 registerDCFCompute(u64 input, const AESBlock* keys,
                                        int depth, int party, 
                                        AESSharedContext* aes_ctx) {
    AESBlock state = keys[0];
    
    // Unroll small trees for better register usage
    #pragma unroll 8
    for (int i = 0; i < min(depth, 8); i++) {
        u8 bit = (input >> (depth - 1 - i)) & 1;
        state = warpDCFTraversal(state, bit, keys[i + 1], party, aes_ctx);
    }
    
    return state & 1;
}

// Shared memory DCF key caching for repeated access
template<int CACHE_SIZE>
__device__ inline void cacheDCFKeys(const AESBlock* global_keys, int offset) {
    __shared__ AESBlock key_cache[CACHE_SIZE];
    int tid = threadIdx.x;
    
    // Load keys cooperatively
    for (int i = tid; i < CACHE_SIZE; i += blockDim.x) {
        key_cache[i] = global_keys[offset + i];
    }
    __syncthreads();
}

} // namespace optimizations
} // namespace dcf