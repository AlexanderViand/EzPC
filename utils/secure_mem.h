// Author: AI Assistant / Claude
// Generated by Claude for GPU-MPC optimizations
// Copyright:
// 
// Copyright (c) 2024 Microsoft Research
// 
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.

#pragma once

#include <cuda.h>
#include <cuda_runtime.h>
#include <string.h>
#include <stdio.h>
#include <stdlib.h>
#include <cassert>
#include "secure_error.h"
#include "helper_cuda.h"

// Forward declaration of CUDA kernel (implemented in gpu_mem.cu)
__global__ void secureZeroKernel(uint8_t* ptr, size_t size);

// Secure GPU memory free function that zeros memory before freeing
inline void gpuFreeSecure(void* d_ptr, size_t size_bytes) {
    if (d_ptr == nullptr) {
        return;
    }
    
    // Zero out the memory before freeing
    const int block_size = 256;
    int grid_size = (size_bytes + block_size - 1) / block_size;
    
    secureZeroKernel<<<grid_size, block_size>>>((uint8_t*)d_ptr, size_bytes);
    checkCudaErrorsSecure(cudaDeviceSynchronize());
    
    // Now free the memory
    checkCudaErrorsSecure(cudaFreeAsync(d_ptr, 0));
}

// Secure CPU memory free function that zeros memory before freeing
inline void cpuFreeSecure(void* h_ptr, size_t size_bytes, bool pinned = true) {
    if (h_ptr == nullptr) {
        return;
    }
    
    // Zero out the memory before freeing
    memset(h_ptr, 0, size_bytes);
    
    // Unregister pinned memory if needed
    if (pinned) {
        checkCudaErrorsSecure(cudaHostUnregister(h_ptr));
    }
    
    free(h_ptr);
}

// Template class for secure memory management
template<typename T>
class SecureGpuMemory {
private:
    T* d_ptr;
    size_t count;
    size_t size_bytes;
    bool is_allocated;
    
public:
    SecureGpuMemory() : d_ptr(nullptr), count(0), size_bytes(0), is_allocated(false) {}
    
    // Allocate GPU memory
    bool allocate(size_t element_count) {
        if (is_allocated) {
            secureLogError("SecureGpuMemory::allocate", -1);
            return false;
        }
        
        count = element_count;
        size_bytes = count * sizeof(T);
        
        checkCudaErrorsSecure(cudaMallocAsync(&d_ptr, size_bytes, 0));
        
        if (d_ptr != nullptr) {
            is_allocated = true;
            return true;
        }
        return false;
    }
    
    // Get pointer (const version for safety)
    const T* get() const {
        return d_ptr;
    }
    
    // Get mutable pointer (use with caution)
    T* getMutable() {
        return d_ptr;
    }
    
    // Get size information
    size_t getCount() const { return count; }
    size_t getSizeBytes() const { return size_bytes; }
    
    // Secure zero the allocated memory
    void secureZero() {
        if (!is_allocated || d_ptr == nullptr) {
            return;
        }
        
        const int block_size = 256;
        int grid_size = (size_bytes + block_size - 1) / block_size;
        
        secureZeroKernel<<<grid_size, block_size>>>((uint8_t*)d_ptr, size_bytes);
        checkCudaErrorsSecure(cudaDeviceSynchronize());
    }
    
    // Destructor - automatically performs secure cleanup
    ~SecureGpuMemory() {
        if (is_allocated && d_ptr != nullptr) {
            gpuFreeSecure(d_ptr, size_bytes);
        }
    }
    
    // Move constructor
    SecureGpuMemory(SecureGpuMemory&& other) noexcept 
        : d_ptr(other.d_ptr), count(other.count), size_bytes(other.size_bytes), 
          is_allocated(other.is_allocated) {
        other.d_ptr = nullptr;
        other.is_allocated = false;
    }
    
    // Move assignment
    SecureGpuMemory& operator=(SecureGpuMemory&& other) noexcept {
        if (this != &other) {
            // Clean up current memory
            if (is_allocated && d_ptr != nullptr) {
                gpuFreeSecure(d_ptr, size_bytes);
            }
            
            // Transfer ownership
            d_ptr = other.d_ptr;
            count = other.count;
            size_bytes = other.size_bytes;
            is_allocated = other.is_allocated;
            
            other.d_ptr = nullptr;
            other.is_allocated = false;
        }
        return *this;
    }
    
    // Delete copy constructor and assignment to prevent accidental copying
    SecureGpuMemory(const SecureGpuMemory&) = delete;
    SecureGpuMemory& operator=(const SecureGpuMemory&) = delete;
};

// Template class for secure CPU memory management
template<typename T>
class SecureCpuMemory {
private:
    T* h_ptr;
    size_t count;
    size_t size_bytes;
    bool is_allocated;
    bool is_pinned;
    
public:
    SecureCpuMemory(bool pin = true) : h_ptr(nullptr), count(0), size_bytes(0), 
                                       is_allocated(false), is_pinned(pin) {}
    
    // Allocate CPU memory
    bool allocate(size_t element_count) {
        if (is_allocated) {
            secureLogError("SecureCpuMemory::allocate", -1);
            return false;
        }
        
        count = element_count;
        size_bytes = count * sizeof(T);
        
        int err = posix_memalign((void**)&h_ptr, 32, size_bytes);
        if (err != 0) {
            secureLogError("SecureCpuMemory::posix_memalign", err);
            return false;
        }
        
        if (is_pinned) {
            checkCudaErrorsSecure(cudaHostRegister(h_ptr, size_bytes, cudaHostRegisterDefault));
        }
        
        is_allocated = true;
        return true;
    }
    
    // Get pointer (const version for safety)
    const T* get() const {
        return h_ptr;
    }
    
    // Get mutable pointer (use with caution)
    T* getMutable() {
        return h_ptr;
    }
    
    // Get size information
    size_t getCount() const { return count; }
    size_t getSizeBytes() const { return size_bytes; }
    
    // Secure zero the allocated memory
    void secureZero() {
        if (!is_allocated || h_ptr == nullptr) {
            return;
        }
        memset(h_ptr, 0, size_bytes);
    }
    
    // Destructor - automatically performs secure cleanup
    ~SecureCpuMemory() {
        if (is_allocated && h_ptr != nullptr) {
            cpuFreeSecure(h_ptr, size_bytes, is_pinned);
        }
    }
    
    // Move constructor
    SecureCpuMemory(SecureCpuMemory&& other) noexcept 
        : h_ptr(other.h_ptr), count(other.count), size_bytes(other.size_bytes),
          is_allocated(other.is_allocated), is_pinned(other.is_pinned) {
        other.h_ptr = nullptr;
        other.is_allocated = false;
    }
    
    // Move assignment
    SecureCpuMemory& operator=(SecureCpuMemory&& other) noexcept {
        if (this != &other) {
            // Clean up current memory
            if (is_allocated && h_ptr != nullptr) {
                cpuFreeSecure(h_ptr, size_bytes, is_pinned);
            }
            
            // Transfer ownership
            h_ptr = other.h_ptr;
            count = other.count;
            size_bytes = other.size_bytes;
            is_allocated = other.is_allocated;
            is_pinned = other.is_pinned;
            
            other.h_ptr = nullptr;
            other.is_allocated = false;
        }
        return *this;
    }
    
    // Delete copy constructor and assignment to prevent accidental copying
    SecureCpuMemory(const SecureCpuMemory&) = delete;
    SecureCpuMemory& operator=(const SecureCpuMemory&) = delete;
};

// Note: C-style secure allocation functions are implemented in gpu_mem.cu
// and declared in gpu_mem.h to maintain compatibility with existing code